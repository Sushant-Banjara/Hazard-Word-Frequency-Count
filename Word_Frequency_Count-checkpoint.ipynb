{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "##Import the required libraries\n",
    "import itertools ##Iterations\n",
    "import nltk ##natural language processing\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import os ##Operating System Dependent functionality\n",
    "import sys ##Info on python interpreter\n",
    "import glob\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns ##Data visualisation\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "from datetime import datetime\n",
    "%matplotlib inline\n",
    "# run for jupyter notebook\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "\n",
    "print (stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Sushant\\\\Documents\\\\newspaper-word-count'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = 'C:/Users/Sushant/Documents/newspaper-word-count'\n",
    "os.chdir(root)\n",
    "%pwd() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = root + '/tmp'\n",
    "inpt = root + '/raw_txts'\n",
    "output = root + '/word_frequencies/'\n",
    "\n",
    "if not os.path.exists(output):\n",
    "    os.makedirs(output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_word_freqs(inputfile,outdir):\n",
    "    Remove= set(stopwords.words('english')+\n",
    "               list(string.punctuation)+\n",
    "               ['\\'\\'','``','\\'s','’','“',\"”\",\n",
    "                'the','said','nepal','world','kathmandu'])\n",
    "    cols = ['word','freq']\n",
    "\n",
    "    base = os.path.abspath(inputfile)\n",
    "    wdir, fname = outdir, os.path.split(base)[1]\n",
    "    writepath = wdir + '/wfreqs_' + fname.split('.')[0] + '.csv'\n",
    "\n",
    "    f = open(inputfile, errors = 'ignore')\n",
    "    raw = f.read()\n",
    "    tokens = [token.lower() for token in nltk.word_tokenize(raw)]\n",
    "    cleaned = [token for token in tokens if token not in Remove]\n",
    "    \n",
    "    fdict = dict(nltk.FreqDist(cleaned))\n",
    "    df = pd.DataFrame(list(fdict.items()),columns=cols)\n",
    "    df = df.sort_values('freq',ascending=0)\n",
    "    \n",
    "    df.to_csv(writepath,columns=['word','freq'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nltk.download() Download the NLTK Data and add the path of NLTK data to rest of the nltk data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\Sushant/nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data', 'C:\\\\Users\\\\Sushant\\\\Anaconda3\\\\nltk_data', 'C:\\\\Users\\\\Sushant\\\\Anaconda3\\\\share\\\\nltk_data', 'C:\\\\Users\\\\Sushant\\\\Anaconda3\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\Sushant\\\\AppData\\\\Roaming\\\\nltk_data', 'C://Users/Sushant/AppData/Roaming/nltk_data/']\n"
     ]
    }
   ],
   "source": [
    "nltk.data.path.append('C://Users/Sushant/AppData/Roaming/nltk_data/')\n",
    "print (nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Sushant/Documents/newspaper-word-count/raw_txts\n"
     ]
    }
   ],
   "source": [
    "# pick file, remove punctuation and stopwords\n",
    "temp = root + '/tmp'\n",
    "inpt = root + '/raw_txts'\n",
    "print(inpt)\n",
    "output = root + '/word_frequencies/'\n",
    "\n",
    "if not os.path.exists(output):\n",
    "    os.makedirs(output)\n",
    "    \n",
    "files = glob.glob(inpt+'/TKP_*.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in files: \n",
    "    write_word_freqs(i,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_stitcher(prefix,ybegin, yend,\n",
    "                  months = ['{num:02d}'.format(num=x) for x in range(1, 13)], \n",
    "                  days= ['{num:02d}'.format(num=x) for x in range(1, 32)] ):\n",
    "    years = [str(x) for x in range(ybegin, yend)]\n",
    "    filelist = []\n",
    "    combinations = list(itertools.product(years, months, days))\n",
    "    for combination in combinations:\n",
    "        arguments = \"_\".join(combination)\n",
    "        command = prefix + arguments + '.csv'\n",
    "        filelist.append(command)\n",
    "    return filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kw_search(flist, keywords, inpdir,fuzzymatch=False,verbose=False):\n",
    "    \n",
    "    freqcols = ['word', 'freq']\n",
    "    # initialise dataframe\n",
    "    tallies = pd.DataFrame(flist, columns=['file']).set_index('file')\n",
    "    tallies['exists']=np.nan\n",
    "    tallies['wordcount']=np.nan\n",
    "    for kw in keywords:\n",
    "        tallies[kw] = np.nan\n",
    "    # count in all files\n",
    "    for infile in flist:\n",
    "    \n",
    "        file = inpdir + '/' + infile\n",
    "        exist_flag = os.path.exists(file)\n",
    "        if exist_flag:\n",
    "            tallies.at[infile, 'exists'] = 1\n",
    "            tkp = pd.read_csv(file, usecols=freqcols, index_col='word')\n",
    "            tallies.at[infile, 'wordcount'] = tkp.freq.sum()\n",
    "            \n",
    "            for kw in keywords:\n",
    "                try:\n",
    "                    tallies.at[infile, kw] = pd.to_numeric(tkp.loc[kw])[0]\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "            if verbose: print(infile, 'loaded and searched')\n",
    "            del tkp\n",
    "        else:\n",
    "            tallies.at[infile, 'exists'] = 0\n",
    "            if verbose: print(infile, ' does not exist')\n",
    "            continue\n",
    "    # subset to nonempty rows\n",
    "    data = tallies[(tallies['exists']==1)]\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_ngram_data(flist,keywords,inp,fuzzymatch=False,percent=False):\n",
    "    data = kw_search(flist, keywords, inp, fuzzymatch)\n",
    "    # this will print a barrage of warnings \n",
    "    data.reset_index(level=0, inplace=True)\n",
    "    data.file = data.file.str[11:21] # hacky - relies on particular naming format\n",
    "    data['date']=pd.to_datetime(data['file'], format='%Y_%m_%d')\n",
    "    keepvars = ['date','wordcount']+keywords\n",
    "    \n",
    "    # normalize by day word-count \n",
    "    if percent:\n",
    "        #for k in keywords:\n",
    "            #data[k] = (data[k]/data['wordcount'])*100\n",
    "        for k in keywords:\n",
    "            data[k] = data[k]\n",
    "    clean = data[keepvars].set_index('date')\n",
    "    \n",
    "    return(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_interactive(df,vars,header='Appearances in TKP archive'):\n",
    "    data = []\n",
    "    for v in vars:\n",
    "        data.append(go.Scatter(x = df.index,y = df[v],name=v))\n",
    "    \n",
    "    layout = dict(\n",
    "        title=header,\n",
    "        xaxis=dict(\n",
    "            rangeselector=dict(\n",
    "                buttons=list([\n",
    "                    dict(count=1,label='1m',step='month',stepmode='backward'),\n",
    "                    dict(count=6,label='6m',step='month',stepmode='backward'),\n",
    "                    dict(count=1,label='YTD',step='year',stepmode='todate'),\n",
    "                    dict(count=1,label='1y',step='year',stepmode='backward'),\n",
    "                    dict(count=2,label='2y',step='year',stepmode='backward'),\n",
    "                    dict(step='all')\n",
    "                ])\n",
    "            ),\n",
    "            rangeslider=dict(),\n",
    "            type='date'\n",
    "        )\n",
    "    )\n",
    "    fig = dict(data=data, layout=layout)\n",
    "    return py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preliminaries\n",
    "working = 'C:/Users/Sushant/Documents/newspaper-word-count'\n",
    "os.chdir(working)\n",
    "tmp = 'C:/tmp/'\n",
    "inp = working + '/word_frequencies'\n",
    "\n",
    "# set up list for date ranges\n",
    "flist = date_stitcher('wfreqs_TKP_',2008, 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "disasters = ['earthquake', 'landslide', 'fire', 'flood', 'drought', 'rain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "stressors = prep_ngram_data(flist,disasters,inp,fuzzymatch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotly graphs are hosted by Plotly website, so you need to import plotly using above command\n",
    "\n",
    "import plotly\n",
    "\n",
    "plotly.tools.set_credentials_file(username='yourusername', api_key='yourapikey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Suxant/2.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_notebook_mode(connected='TRUE')\n",
    "plot_interactive(stressors,disasters,header='word count The Kathmandu Post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            wordcount  earthquake  landslide  fire  flood  drought  rain\n",
      "date                                                                    \n",
      "2009-01-01    16206.0         NaN        2.0   6.0    NaN      NaN   1.0\n",
      "2009-01-02    16315.0         NaN        NaN   9.0    NaN      NaN   NaN\n",
      "2009-01-03    14816.0         1.0        3.0   5.0    1.0      NaN   NaN\n",
      "2009-01-04    14400.0         6.0        NaN   4.0    NaN      NaN   NaN\n",
      "2009-01-06    15800.0         NaN        NaN   5.0    2.0      NaN   NaN\n",
      "2009-01-07    16138.0         NaN        2.0  10.0    2.0      NaN   NaN\n",
      "2009-01-08    13899.0         NaN        NaN   1.0    7.0      NaN   NaN\n",
      "2009-01-09    14329.0         NaN        NaN   4.0    NaN      NaN   NaN\n",
      "2009-01-10    14628.0         NaN        NaN   2.0    2.0      NaN   NaN\n",
      "2009-01-11    14287.0         NaN        NaN   2.0    NaN      NaN   3.0\n",
      "2009-01-12    14499.0         NaN        NaN   1.0    3.0      NaN   NaN\n",
      "2009-01-13    12930.0         NaN        NaN   4.0    NaN      NaN   NaN\n",
      "2009-01-14    15694.0         3.0        NaN   7.0    NaN      NaN   3.0\n",
      "2009-01-15    15252.0         3.0        1.0   4.0    NaN      NaN   3.0\n",
      "2009-01-16    14926.0        12.0        NaN   2.0    1.0      NaN   1.0\n",
      "2009-01-17    15208.0         NaN        NaN   NaN    NaN      NaN   1.0\n",
      "2009-01-18    14290.0         NaN        NaN   2.0    NaN      NaN   NaN\n",
      "2009-01-19    14730.0         NaN        NaN   4.0    1.0      NaN   NaN\n",
      "2009-01-20    14629.0         NaN        NaN   8.0    1.0      NaN   NaN\n",
      "2009-01-21    15083.0         NaN        NaN   5.0    2.0      NaN   NaN\n",
      "2009-01-22    15655.0         NaN        NaN   1.0    NaN      NaN   NaN\n",
      "2009-01-23    13874.0         NaN        NaN   4.0    2.0      NaN   NaN\n",
      "2009-01-25    14939.0         NaN        NaN   4.0    NaN      NaN   NaN\n",
      "2009-01-26    11035.0         NaN        NaN   4.0    NaN      NaN   NaN\n",
      "2009-01-27    13095.0         NaN        NaN   6.0    2.0      NaN   NaN\n",
      "2009-01-30    12956.0         NaN        1.0   2.0    2.0      NaN   3.0\n",
      "2009-01-31    14768.0         NaN        NaN   2.0    NaN      NaN   NaN\n",
      "2009-02-01    14315.0         1.0        NaN   7.0    NaN      NaN   1.0\n",
      "2009-02-02    13723.0         NaN        NaN   6.0    NaN      NaN   NaN\n",
      "2009-02-03    15597.0         NaN        NaN   6.0    2.0      NaN   NaN\n",
      "...               ...         ...        ...   ...    ...      ...   ...\n",
      "2012-10-22    18072.0         NaN        NaN   7.0    1.0      NaN   1.0\n",
      "2012-10-27    16910.0         NaN        NaN   2.0    2.0      1.0   4.0\n",
      "2012-10-28    19009.0         1.0        NaN   7.0    1.0      NaN   1.0\n",
      "2012-10-29    19412.0         NaN        NaN   2.0    NaN      NaN   1.0\n",
      "2012-10-30    19505.0         1.0        NaN   NaN    2.0      NaN   2.0\n",
      "2012-10-31    19854.0         NaN        NaN   3.0    2.0      NaN   NaN\n",
      "2012-11-01    19445.0         1.0        NaN   7.0    2.0      NaN   3.0\n",
      "2012-11-02    19142.0         1.0        NaN   NaN    NaN      NaN   NaN\n",
      "2012-11-04    17519.0         NaN        NaN   3.0    1.0      NaN   NaN\n",
      "2012-11-05    19345.0         NaN        NaN   NaN    1.0      NaN   1.0\n",
      "2012-11-06    19063.0         1.0        NaN   NaN    NaN      1.0   2.0\n",
      "2012-11-07    18898.0         NaN        NaN   3.0    1.0      NaN   1.0\n",
      "2012-11-08    19683.0         NaN        NaN   3.0    NaN      1.0   2.0\n",
      "2012-11-09    17359.0         2.0        5.0   3.0    NaN      NaN   1.0\n",
      "2012-11-10    17828.0         NaN        1.0   NaN    NaN      1.0   NaN\n",
      "2012-11-12    18744.0         1.0        1.0   2.0    7.0      NaN   NaN\n",
      "2012-11-13    19684.0        27.0        NaN  11.0    1.0      NaN   2.0\n",
      "2012-11-14    17341.0         1.0        NaN   3.0    NaN      NaN   1.0\n",
      "2012-11-17    19144.0         NaN        NaN  25.0    NaN      NaN   1.0\n",
      "2012-11-18    19951.0         7.0        NaN   5.0    1.0      NaN   NaN\n",
      "2012-11-19    20659.0         NaN        NaN   6.0    NaN      NaN   NaN\n",
      "2012-11-20    21654.0         2.0        NaN   4.0    NaN      1.0   NaN\n",
      "2012-11-21    20359.0         NaN        NaN   7.0    NaN      1.0   1.0\n",
      "2012-11-22    18967.0         NaN        NaN   5.0    NaN      NaN   1.0\n",
      "2012-11-23    18897.0         1.0        NaN   3.0    NaN      NaN   NaN\n",
      "2012-11-24    19664.0         NaN        NaN   2.0    NaN      NaN   6.0\n",
      "2012-11-25    19827.0         NaN        1.0   1.0    NaN      NaN   NaN\n",
      "2012-11-26    19704.0         NaN        NaN  24.0    2.0      NaN   1.0\n",
      "2012-11-27    21841.0         NaN        NaN   3.0    NaN      NaN   1.0\n",
      "2012-11-28    20937.0         NaN        NaN  11.0    NaN      NaN   NaN\n",
      "\n",
      "[1326 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(stressors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
